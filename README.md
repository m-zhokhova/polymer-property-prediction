# Polymer Property Prediction: GNN vs Tree-Based Models

A comparison of **Graph Neural Networks** and **Tree-Based Models** (ExtraTreesRegressor) for predicting polymer properties from molecular structure (SMILES).

Based on the [NeurIPS Open Polymer Prediction 2025](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025) Kaggle competition.

---

## ğŸ¯ Problem

Predict 5 physical properties of polymers from their SMILES representation:

| Property | Description | Unit |
|----------|-------------|------|
| **Tg** | Glass Transition Temperature | Kelvin |
| **FFV** | Fractional Free Volume | - |
| **Tc** | Thermal Conductivity | W/(mÂ·K) |
| **Density** | Mass per Volume | g/cmÂ³ |
| **Rg** | Radius of Gyration | Ã… |

---

## ğŸ”¬ Two Approaches Compared

### 1. Graph Neural Network (GNN)
- Treats molecules as graphs (atoms = nodes, bonds = edges)
- 4-layer GINE architecture with residual connections
- 18 atom features + 7 bond features
- Multi-task learning (predicts all 5 targets simultaneously)

### 2. Tree-Based Models (ExtraTrees + ensemble)
- Uses RDKit molecular descriptors (~200 features)
- Morgan fingerprints (512 bits)
- Ensemble of ExtraTrees, GradientBoosting, RandomForest
- Separate model per target

---

## ğŸ“Š Results

| Target | GNN MAE | Tree MAE | Winner | Improvement |
|--------|---------|----------|--------|-------------|
| **Tg** | 46.05 K | 53.31 K | GNN | +13.6% |
| **FFV** | 0.0049 | 0.0069 | GNN | +28.6% |
| **Tc** | 0.0296 | 0.0293 | Tree | +1.1% |
| **Density** | 0.0294 | 0.0325 | GNN | +9.5% |
| **Rg** | 1.40 | 1.73 | GNN | +18.9% |

**GNN wins 4 out of 5 targets**, but tree models remain competitive â€” especially considering they train in seconds vs. minutes for GNN.

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/polymer-property-prediction.git
cd polymer-property-prediction

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Running the Notebook

```bash
jupyter notebook polymer_gnn_vs_trees.ipynb
```

Or run directly on [Kaggle](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025) where the data is available.

---

## ğŸ“ Project Structure

```
polymer-property-prediction/
â”‚
â”œâ”€â”€ polymer_gnn_vs_trees.ipynb   # Main comparison notebook
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # This file
â”‚
â””â”€â”€ data/                        # Data directory (not included)
    â”œâ”€â”€ train.csv
    â””â”€â”€ test.csv
```

### Data

The dataset is from the Kaggle competition. To run locally:
1. Download from [Kaggle](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025/data)
2. Place `train.csv` and `test.csv` in the `data/` folder
3. Update file paths in the notebook

---

## ğŸ”‘ Key Findings

1. **GNN excels at structural properties** â€” Tg and Rg depend heavily on molecular topology, which GNNs capture naturally through message passing.

2. **Tree models are surprisingly competitive** â€” With good feature engineering (RDKit + Morgan fingerprints), they achieve ~80-90% of GNN performance.

3. **Data scarcity hurts both** â€” Only 667 samples for Tg resulted in ~46K MAE for both approaches. More data would help significantly.

4. **Ensemble is best** â€” Combining GNN (40%) + Trees (60%) gives more robust predictions than either alone.

---

## ğŸ“š Methods Explained

### ExtraTreesRegressor

Extremely Randomized Trees â€” a variant of Random Forest that uses random splits instead of optimal splits:

```python
from sklearn.ensemble import ExtraTreesRegressor

model = ExtraTreesRegressor(
    n_estimators=200,
    max_depth=15,
    random_state=42,
    n_jobs=-1
)
```

**Why it works well here:**
- Handles high-dimensional sparse features (Morgan fingerprints)
- Robust to outliers
- Fast training
- No need for feature scaling

### GNN Architecture

```
Input â†’ NodeEmbed â†’ [GINE + LayerNorm + Residual] Ã— 4 â†’ MultiPooling â†’ MLP â†’ Output
```

- **GINE**: Graph Isomorphism Network with Edge features
- **Multi-pooling**: Concatenates mean, max, and sum pooling
- **Residual connections**: Helps with gradient flow

---

## ğŸ› ï¸ Possible Improvements

- [ ] Add external datasets for Tg and Tc
- [ ] Try XGBoost / LightGBM
- [ ] Implement 5-fold cross-validation
- [ ] Use pretrained molecular embeddings (ChemBERTa)
- [ ] Hyperparameter tuning with Optuna
- [ ] Add 3D molecular coordinates

---

## ğŸ“– Related Article

*Coming soon on Medium â€” a deep dive into ExtraTreesRegressor and when "boring" models win.*

---

## ğŸ“œ License

MIT License â€” feel free to use, modify, and share.

---

## ğŸ‘©â€ğŸ’» Author

**Maria Zhokhova**  
Data Science MSc @ FCUP/INESC-TEC  
[LinkedIn](https://www.linkedin.com/in/m-zhokhova/)

---

## ğŸ™ Acknowledgments

- [NeurIPS Open Polymer Prediction 2025](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025) competition organizers
- [RDKit](https://www.rdkit.org/) for molecular descriptors
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/) for GNN implementation
